# Process ZMQ events in near real-time with Apache Flink

**When you subscribe to ZMQ events, you receive near real-time Tangle data from a node. To process this data, you can use a stream processing framework such as the open-source [Apache Flink](https://flink.apache.org/).**

This guide uses the [Flink Tangle source library](https://github.com/Citrullin/flink-tangle-source) to use process ZMQ data with Flink. 

This library uses the [ZeroMQMessageParser](https://github.com/Citrullin/tangle-streaming/blob/master/src/main/scala/org/iota/tangle/stream/ZeroMQMessageParser.scala) from the [Tangle streaming library](https://github.com/Citrullin/tangle-streaming) to parse the raw event messages into class instances.
All ZMQ event messages are wrapped in classes that are generated by [protobuf schema files](https://github.com/Citrullin/tangle-streaming/tree/master/src/main/protobuf). All protobuf messages and attributes are also available in Flink.

Because this library uses the ZMQ API, all [ZMQ events](../references/zmq-events.md) are available for processing.

:::info:
The Tangle streaming libraries in this tutorial are not recommended for production environments.
Feel free to contribute to the libraries, so that they eventually become production ready.
:::

## Prerequisites

To complete this tutorial, you need the following:

- **Operating system:** Linux, MacOS, BSD or Windows
- **RAM:** 2GB
- **Storage:** 10GB free space

This guide uses the Scala programming language with the sbt build tool.

If you want to use Scala in a Java Runtime Environment (JRE), you need to add the Scala library to [Maven](https://mvnrepository.com/artifact/org.scala-lang/scala-library) or [sbt](http://xerial.org/blog/2014/03/24/sbt/).
 
This [Artima guide](https://www.artima.com/pins1ed/combining-scala-and-java.html) describes how you can use Scala in a JRE.

## Download and install the libraries

1. [Install Java](http://openjdk.java.net/install/). Because Scala uses the Java virtual machine, you must install Java 8 or higher.

2. [Install sbt](https://www.scala-sbt.org/1.x/docs/Setup.html)

3. Clone the libraries

  ```bash
  git clone https://github.com/Citrullin/tangle-streaming.git
  git clone https://github.com/Citrullin/flink-tangle-source
  ```

4. Change into the `tangle-streaming` directory and initialize the REPL (Read-Evaluate-Print Loop)

  ```bash
  cd tangle-streaming && sbt
  ```

5. In the REPL, build the library

  ```bash
  compile
  publishLocal
  ```

6. Press **Ctrl** + **C** to terminate the REPL

7. Change into the `flink-tangle-source` directory and initialize the REPL

  ```bash
  cd ../flink-tangle-source && sbt
  ```

8. In the REPL, build the library

  ```bash
  compile
  publishLocal
  ```

9. Add the dependencies to the `build.sbt` file

  ```scala
  libraryDependencies += "org.iota" %% "flink-tangle-source" % "0.0.1",
  ```

:::success:
Now that you've downloaded and installed the libraries you can start using them to process ZMQ data.
We have [some examples available here](https://github.com/iota-community/flink-tangle-examples).
:::

:::info:
If you run your own IRI node, you have to [enable the ZMQ configuration parameter](../references/iri-configuration-options.md).

[Tanglebeat provides a list of public nodes that have ZMQ enabled.](http://tanglebeat.com/page/internals).

cIRI does not support the ZMQ API at the moment.
:::

## Process the top 10 most used addresses in the last hour

You can use the data in ZMQ event streams to find out the top 10 most used addresses in the last hour.

This code in this tutorial is available in the `MostUsedAddresses.scala` file on [this IOTA community GitHub repository](https://github.com/iota-community/flink-tangle-examples).

## Prerequisites

If you are not familiar with Flink, you should read [this documentation](https://ci.apache.org/projects/flink/flink-docs-master/how-to-guides/datastream_api.html#writing-a-flink-program).

You must have [downloaded and installed the libraries](#download-and-install-the-libraries).

---

Set up the stream by connecting to a node

```scala
val unconfirmedMessageDescriptorName = UnconfirmedTransactionMessage.scalaDescriptor.fullName
val zeroMQHost = "HOSTNAME|IP"
val zeroMQPort = config.getInt(ConfigurationKeys.ZeroMQ.port)
val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

val stream = env.addSource(new TangleSource(zeroMQHost, zeroMQPort, ""))
```
:::info:
Here, we connect to a node by its hostname and port. We could subscribe to a specific topic such as the [tx](../references/zmq-events.md#tx) event:

```scala
val stream = env.addSource(new TangleSource(zeroMQHost, zeroMQPort, "tx"))
```
:::

Since we get a stream of GeneratedMessage, we need to filter with the [protobuf descriptor](https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.descriptor). 

```scala
val filteredStream = stream
  .filter(_.companion.scalaDescriptor.fullName == unconfirmedMessageDescriptorName)
```

We can make sure with this that the stream only contains UnconfirmedTransactionMessages.
So now we check the type, wrap it in an option and getting the value. 
Since we already filtered on the descriptor, we know that every event is of type UnconfirmedTransactionMessage.
If not, something fundamental is wrong and a NullPointerException will crash the application.

```scala
val unconfirmedTransactionStream = filteredStream.map(_ match {
        case m: UnconfirmedTransactionMessage => Some(m)
        case _ => None
      })
      .map(_.get)
```

This is a uncommon and dirty way to do. You should never use get, since you can run into `NullPointer` exceptions.
Use [getOrElse](https://www.tutorialspoint.com/scala/scala_options.htm) instead. 
It would also make sense to implement a filter into the library, so that the correct type is returned.
That would make the type checking obsolete.
Since this library is just a proof of concept, we go with this dirty solution for now.

Now we have our stream of the type UnconfirmedTransactionMessage. We basically get every message our full-node receives.
We want to find out which addresses were used the most. That means, we only need the address and some counter.
For simplicity we count every address in a transaction as one. We could also only keep the inputs. 
To detect double used addresses, we can also filter on outputs. 
If you want to do that, you have to apply a filter with value > 0 or value < 0.

```scala
val addressOnlyStream = unconfirmedTransactionStream.map(e => (e.address, 1L))
```

Simple as that. We change the structure of our element with this simple map function.
We only keep the address and a counter. [Tuples](https://docs.scala-lang.org/tour/tuples.html) are useful for this.

Since we want to count our elements, we can key our stream by the address. 
This gives us a KeyedStream partitioned by the address. For more complex use-cases you can use [windowAll](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/windows.html#window-assigners)

```scala
val keyedStream = addressOnlyStream.keyBy(_._1)
```
 
So, now we have a KeyedStream where every partition only contain some tuples of the same kind. 
Basically a lot of tuples with the same address and 1L.
Partitioning is useful if you want to process a huge amount of data.
Flink can execute the processor for each partition on different nodes in your cluster, so that the process functions
on each stream can work in parallel. Therefore you are able to scale horizontally.

Next, we need to calculate the number of transactions for each address within one hour. 
Sliding Windows are useful for this. An update interval of 30 seconds is fine for our use-case.

```scala
val keyedTimedWindow = keyedStream.timeWindow(Time.minutes(60), Time.seconds(30))
```

We got our keyedTimeWindows. Now we need to aggregate our partitions. 
We have two options for this. The simplest variant is the reduce function. 
This is a function which reduces all elements to the few we really need. 
In our case, this would be our reduce function:

```scala
val aggregatedKeyedTimeWindow = timedWindow.reduce((a, b) => (a._1, a._2 + b._2))
```

Simple as that. The other variant is an aggregation function. One example:

```scala
val aggregatedKeyedTimeWindow = keyedTimedWindow.aggregate(new AddressCountAggregator)
```

The AddressCountAggregator class

```scala
class AddressCountAggregator extends AggregateFunction[(String, Long), (String, Long), (String, Long)]
{
  override def add(value: (String, Long), accumulator: (String, Long)): (String, Long) =
    (value._1, value._2 + accumulator._2)

  override def createAccumulator(): (String, Long) = ("", 0L)

  override def getResult(accumulator: (String, Long)): (String, Long) = accumulator

  override def merge(a: (String, Long), b: (String, Long)): (String, Long) = (a._1, a._2 + b._2)
}
```

The reduce function is used whenever you just need to reduce your result. Sums are a good example. 
Therefore in our case the reduce function makes more sense than the aggregation function.
Aggregation functions are helpful when you have complex operations. 
You can find one more complex example in [BundleAggregation.scala](https://github.com/iota-community/flink-tangle-examples/blob/master/src/main/scala/org/iota/tangle/flink/examples/BundleAggregation.scala).
The BundleAggregation combines incoming transaction into a Bundle and split them into UnconfirmedBundles
and ReattachedUnconfirmedBundles. This example is a simplification and does not split the Bundles in an accurate way.

Next we want to aggregate all elements and want to find the top ten addresses.
The timeWindowAll functions returns a AllWindowedStream. So all elements are combined in one stream again. 
Since we used a SlidingWindow on our partitions before, the time here is not that important anymore. 
So, we just use one second.

```scala
val timeWindowAll = aggregatedKeyedTimeWindow
      .timeWindowAll(Time.seconds(1))
```

Our AllWindowedStream contains all reduced partitions in a tuple. 
Each partition has one tuple in the structure (ADDRESS, AMOUNT_OF_TRANSACTIONS).
The last step is to find out which addresses are used the most. So we use an aggregation function for this.

```scala
val mostUsedStream = timeWindowAll.aggregate(new MostUsedAddressesAggregator(10))
```

The MostUsedAddressesAggregator class

```scala
class MostUsedAddressesAggregator(number: Int) extends AggregateFunction[(String, Long), Map[String, Long], List[(String, Long)]]
{
  override def add(value: (String, Long), accumulator: Map[String, Long]): Map[String, Long] = {
    accumulator ++ Map(value._1 -> (value._2 + accumulator.getOrElse(value._1, 0L)))
  }

  override def createAccumulator(): Map[String, Long] = Map()

  override def getResult(accumulator: Map[String, Long]): List[(String, Long)] =
    accumulator.toList.sortWith(_._2 > _._2).take(number)

  override def merge(a: Map[String, Long], b: Map[String, Long]): Map[String, Long] = {
    val seq = a.toSeq ++ b.toSeq
    val grouped = seq.groupBy(_._1)
    val mapWithCounts = grouped.map{case (key, value) => (key, value.map(_._2))}

    mapWithCounts.map{case (key, value) => (key, value.sum)}
  }
}
```

We use a Map as accumulator. Maps are really useful, since they contain key value pairs. 
AggregateFunction returns a sorted List. From the top used address to the bottom one. 
We are only interested in the first ten, so we only take the first 10. 
The constructor of the class takes the number for it.

The last step is simple, print the List and execute our program.

```scala
mostUsedStream.print()

    // execute program
    env.execute("Most used addresses")
```

